{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5505af4",
   "metadata": {},
   "source": [
    "# PRÁCTICA 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f4a760",
   "metadata": {},
   "source": [
    "Lucía Pérez González, Manuel Ramallo Blanco, Alexandre Lorenzo Martínez"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b54141",
   "metadata": {},
   "source": [
    "## 1 - Preprocesado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbe9e71",
   "metadata": {},
   "source": [
    "### 1.1 - Eliminación de duplicados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3b5f96fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abrir datasets\n",
    "import pandas as pd\n",
    "\n",
    "df_vino = pd.read_csv(\"data/train.csv\")\n",
    "\n",
    "# Eliminación de duplicados, ignorado quality\n",
    "def eliminacion_duplicados(df_vino):\n",
    "    cols = df_vino.columns.drop('quality')\n",
    "    df_vino = df_vino.drop_duplicates(subset=cols)\n",
    "    return df_vino"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa0834f",
   "metadata": {},
   "source": [
    "### 1.2 - Binarización de la calidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "9f3f78ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clasificar_vino(valor): \n",
    "    if valor < 7: \n",
    "        return 0\n",
    "    elif valor >= 7: \n",
    "        return 1\n",
    "def binarizar_calidad(df):\n",
    "    df['calidad'] = df['quality'].apply(clasificar_vino) \n",
    "    df = df.drop(columns=['quality'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219ae80f",
   "metadata": {},
   "source": [
    "### 1.3 - Gestión de valores atípicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "35d8d492",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deteccion de valores atipicos\n",
    "\n",
    "def detectar_atipicos(df_train, df_val, cols_diana):\n",
    "    for col in cols_diana:\n",
    "\n",
    "        # Calculamos IQR y límites\n",
    "        Q1 = df_train[col].quantile(0.25)\n",
    "        Q3 = df_train[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower = Q1 - 6 * IQR\n",
    "        upper = Q3 + 6 * IQR\n",
    "        \n",
    "        # Declaramos condición de atípico y registramos sus posiciones\n",
    "        cond_iqr = (df_train[col] < lower) | (df_train[col] > upper)    #OLLO! Nico recomendara ver os casos 1 a 1 \n",
    "        cond_neg = df_train[col] < 0\n",
    "        cond_atipico = cond_iqr | cond_neg\n",
    "\n",
    "        cond_iqr_val = (df_val[col] < lower) | (df_val[col] > upper)    #OLLO! Nico recomendara ver os casos 1 a 1 \n",
    "        cond_neg_val = df_val[col] < 0\n",
    "        cond_atipico_val = cond_iqr_val | cond_neg_val\n",
    "\n",
    "        # Incrementamos el contador de atípicos por fila en ambos conjuntos\n",
    "        df_train.loc[cond_atipico, col] = pd.NA\n",
    "        df_val.loc[cond_atipico_val, col] = pd.NA\n",
    "\n",
    "\n",
    "    return df_train, df_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c7c55f",
   "metadata": {},
   "source": [
    "#### 1.3.11 - Tratamiento de datos atípicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "02d19cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eliminar_o_imputacion(df_train, df_val, cols_diana, max_atipicos=3, umbral_col=20,  target='calidad'):\n",
    "    # Eliminar filas con 4 o más valores atípicos\n",
    "    # Contamos el número de valores atípicos por fila (NA) y filtramos \n",
    "    df_train = df_train[df_train.isnull().sum(axis=1) < max_atipicos]\n",
    "\n",
    "    # Analisis de valores faltantes por columna\n",
    "    for col in cols_diana:\n",
    "        # Contamos el numero de valores faltantes (NA) en la columna(\n",
    "        num_faltantes = df_train[col].isna().sum()\n",
    "        if num_faltantes/len(df_train) > umbral_col:\n",
    "            df_train = df_train.drop(columns=[col])\n",
    "            df_val = df_val.drop(columns=[col])\n",
    "\n",
    "    # Imputacion de valores: mediana para cada nulo de cada columna\n",
    "    for col in cols_diana:\n",
    "        med = df_train[col].median()\n",
    "        df_train.loc[:, col] = df_train[col].fillna(med)\n",
    "        df_val.loc[:, col] = df_val[col].fillna(med)\n",
    "\n",
    "    return df_train, df_val\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d17c98",
   "metadata": {},
   "source": [
    "#### Normalización y selección de características"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "be145b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalización de los datos\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def estandarizar_train_test(train_df, test_df, target):\n",
    "    columnas = train_df.drop(columns=[target]).select_dtypes(include=\"number\").columns\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "\n",
    "    train_df[columnas] = scaler.fit_transform(train_df[columnas])\n",
    "    test_df[columnas] = scaler.transform(test_df[columnas])\n",
    "\n",
    "    return train_df, test_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "bd31d392",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selección de características con SelectKBest\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "\n",
    "def seleccion_caracteristicas(df, k, target='calidad'):\n",
    "    X = df.drop(columns=[target])\n",
    "    y = df[target]\n",
    "\n",
    "    selector = SelectKBest(score_func=f_regression, k=k)\n",
    "    selector.fit(X, y)\n",
    "\n",
    "    selected_features = X.columns[selector.get_support()]\n",
    "\n",
    "    df_fs = df[selected_features.tolist() + [target]]\n",
    "    return df_fs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5187dd",
   "metadata": {},
   "source": [
    "#### Función que empaqueta el preprocesado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "8f4a658d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocesado(train_df_pre, valid_df_pre, target='calidad'):\n",
    "    df_train = train_df_pre.copy()\n",
    "    df_val = valid_df_pre.copy()\n",
    "    cols_diana = [c for c in train_df_pre.columns if c not in [target]]\n",
    "\n",
    "\n",
    "    # Detección de atípicos en train y valid\n",
    "    df_train, df_val = detectar_atipicos(df_train, df_val, cols_diana)\n",
    "\n",
    "    # Tratamiento de atípicos: eliminación o imputación\n",
    "    df_train, df_val = eliminar_o_imputacion(df_train, df_val, cols_diana)\n",
    "\n",
    "    # Normalización de los datos\n",
    "    df_train, df_val = estandarizar_train_test(df_train, df_val, target)\n",
    "\n",
    "    # Selección de características con SelectKBest\n",
    "    df_train = seleccion_caracteristicas(df_train, k=10, target=target)\n",
    "    df_val = df_val[df_train.columns]\n",
    "    return df_train, df_val \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898266ff",
   "metadata": {},
   "source": [
    "## Entrenamiento del modelo de predicción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2563444f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "c959d463",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "def entrenar_o_cargar(modelo, X_train, y_train, nombre_modelo_archivo):\n",
    "    #Cargar el modelo si existe\n",
    "    if os.path.exists(nombre_modelo_archivo):\n",
    "        with open(nombre_modelo_archivo, \"rb\") as f:\n",
    "            modelo = pickle.load(f)\n",
    "    #Si no existe, entrenar el modelo y guardarlo\n",
    "    else:\n",
    "        modelo.fit(X_train, y_train)\n",
    "        with open(nombre_modelo_archivo, \"wb\") as f:\n",
    "            pickle.dump(modelo, f)\n",
    "    \n",
    "    return modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "ab17ac41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "model_configs = [\n",
    "\n",
    "    # ======================\n",
    "    # kNN\n",
    "    # ======================\n",
    "    #0\n",
    "    {\"name\": \"knn\", \"model\": KNeighborsClassifier, \"params\": {\"n_neighbors\": 3}},\n",
    "    #1\n",
    "    {\"name\": \"knn\", \"model\": KNeighborsClassifier, \"params\": {\"n_neighbors\": 5}},\n",
    "    #2\n",
    "    {\"name\": \"knn\", \"model\": KNeighborsClassifier, \"params\": {\"n_neighbors\": 7}},\n",
    "    #3\n",
    "    {\"name\": \"knn\", \"model\": KNeighborsClassifier, \"params\": {\"n_neighbors\": 9}},\n",
    "\n",
    "    # ======================\n",
    "    # Árbol de Decisión\n",
    "    # ======================\n",
    "    #4\n",
    "    {\"name\": \"decision_tree\", \"model\": DecisionTreeClassifier,\n",
    "    \"params\": {\"max_depth\": 5, \"min_samples_split\": 2, \"min_samples_leaf\": 1}},\n",
    "    #5\n",
    "    {\"name\": \"decision_tree\", \"model\": DecisionTreeClassifier,\n",
    "    \"params\": {\"max_depth\": 5, \"min_samples_split\": 2, \"min_samples_leaf\": 5}},\n",
    "    #6\n",
    "    {\"name\": \"decision_tree\", \"model\": DecisionTreeClassifier,\n",
    "    \"params\": {\"max_depth\": 5, \"min_samples_split\": 10, \"min_samples_leaf\": 1}},\n",
    "    #7\n",
    "    {\"name\": \"decision_tree\", \"model\": DecisionTreeClassifier,\n",
    "    \"params\": {\"max_depth\": 5, \"min_samples_split\": 10, \"min_samples_leaf\": 5}},\n",
    "    #8\n",
    "    {\"name\": \"decision_tree\", \"model\": DecisionTreeClassifier,\n",
    "    \"params\": {\"max_depth\": 10, \"min_samples_split\": 2, \"min_samples_leaf\": 1}},\n",
    "    #9\n",
    "    {\"name\": \"decision_tree\", \"model\": DecisionTreeClassifier,\n",
    "    \"params\": {\"max_depth\": 10, \"min_samples_split\": 2, \"min_samples_leaf\": 5}},\n",
    "    #10\n",
    "    {\"name\": \"decision_tree\", \"model\": DecisionTreeClassifier,\n",
    "    \"params\": {\"max_depth\": 10, \"min_samples_split\": 10, \"min_samples_leaf\": 1}},\n",
    "    #11\n",
    "    {\"name\": \"decision_tree\", \"model\": DecisionTreeClassifier,\n",
    "    \"params\": {\"max_depth\": 10, \"min_samples_split\": 10, \"min_samples_leaf\": 5}},\n",
    "\n",
    "    # ======================\n",
    "    # Regresión Logística\n",
    "    # ======================\n",
    "    #12\n",
    "    {\"name\": \"RegrsionLoxistica\", \"model\": LogisticRegression, \"params\": {\"l1_ratio\": 1, \"penalty\": \"elasticnet\", \"C\": 0.1, \"solver\": \"saga\", \"random_state\": 0}},\n",
    "    #13\n",
    "    {\"name\": \"RegrsionLoxistica\", \"model\": LogisticRegression, \"params\": {\"l1_ratio\": 1, \"penalty\": \"elasticnet\", \"C\": 1.0, \"solver\": \"saga\", \"random_state\": 0}},\n",
    "    #14\n",
    "    {\"name\": \"RegrsionLoxistica\", \"model\": LogisticRegression, \"params\": {\"l1_ratio\": 1, \"penalty\": \"elasticnet\", \"C\": 10.0, \"solver\": \"saga\", \"random_state\": 0}},\n",
    "    #15\n",
    "    {\"name\": \"RegrsionLoxistica\", \"model\": LogisticRegression, \"params\": {\"l1_ratio\": 0, \"penalty\": \"elasticnet\", \"C\": 0.1, \"solver\": \"saga\", \"random_state\": 0}},\n",
    "    #16\n",
    "    {\"name\": \"RegrsionLoxistica\", \"model\": LogisticRegression, \"params\": {\"l1_ratio\": 0, \"penalty\": \"elasticnet\", \"C\": 1.0, \"solver\": \"saga\", \"random_state\": 0}},\n",
    "    #17\n",
    "    {\"name\": \"RegrsionLoxistica\", \"model\": LogisticRegression, \"params\": {\"l1_ratio\": 0, \"penalty\": \"elasticnet\", \"C\": 10.0,\"solver\": \"saga\",\"random_state\": 0}},\n",
    "        \n",
    "    # ======================\n",
    "    # Perceptrón multicapa\n",
    "    # ======================\n",
    "    #18\n",
    "    {\"name\": \"PerceptronMulticapa\", \"model\": MLPClassifier, \"params\": {\"hidden_layer_sizes\": (50,), \n",
    "                                                                       \"learning_rate_init\": 0.001, \"max_iter\": 1500, \"random_state\": 0, \"solver\": \"lbfgs\"}},\n",
    "    \n",
    "    #19\n",
    "    {\"name\": \"PerceptronMulticapa\", \"model\": MLPClassifier, \"params\": {\"hidden_layer_sizes\": (50,), \n",
    "                                                                       \"learning_rate_init\": 0.01, \"max_iter\": 1500, \"random_state\": 0, \"solver\": \"lbfgs\"}},\n",
    "    #20\n",
    "    {\"name\": \"PerceptronMulticapa\", \"model\": MLPClassifier, \"params\": {\"hidden_layer_sizes\": (100,), \n",
    "                                                                       \"learning_rate_init\": 0.001, \"max_iter\": 1500, \"random_state\": 0, \"solver\": \"lbfgs\"}},\n",
    "    #21\n",
    "    {\"name\": \"PerceptronMulticapa\", \"model\": MLPClassifier, \"params\": {\"hidden_layer_sizes\": (100,), \n",
    "                                                                       \"learning_rate_init\": 0.01, \"max_iter\": 1500, \"random_state\": 0, \"solver\": \"lbfgs\"}},   \n",
    "    #22\n",
    "    {\"name\": \"PerceptronMulticapa\", \"model\": MLPClassifier, \"params\": {\"hidden_layer_sizes\": (50, 50), \n",
    "                                                                       \"learning_rate_init\": 0.001, \"max_iter\": 1500, \"random_state\": 0, \"solver\": \"lbfgs\"}},\n",
    "    #23\n",
    "    {\"name\": \"PerceptronMulticapa\", \"model\": MLPClassifier, \"params\": {\"hidden_layer_sizes\": (50, 50), \n",
    "                                                                       \"learning_rate_init\": 0.01, \"max_iter\": 1500, \"random_state\": 0, \"solver\": \"lbfgs\"}}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "2fead309",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo de entrenamiento\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import groupby\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def nested_cv(df, target='calidad'):\n",
    "    \"\"\"\n",
    "    Validación cruzada anidada:\n",
    "    - exterior: 3 iteraciones (6 pedazos, 4 train, 2 val)\n",
    "    - interior: 4 pedazos (3 train, 1 val)\n",
    "    \n",
    "    Devuelve: diccionario con información de folds\n",
    "    \"\"\"\n",
    "    #preprocesado inicial\n",
    "    df = eliminacion_duplicados(df)\n",
    "    df = binarizar_calidad(df)\n",
    "    # Mezclar el dataset para aleatoriedad\n",
    "    df_shuffled = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    n = len(df_shuffled)\n",
    "    \n",
    "    # Dividir en 6 pedazos iguales (outer)\n",
    "    outer_splits = [ df_shuffled.iloc[i::6] for i in range(6) ]\n",
    "    \n",
    "    results = []\n",
    "\n",
    "    # Outer CV\n",
    "    for outer_iter in range(3):\n",
    "        #Se construye el diccionario para cada iteración externa -> se almacena un array de puntuaciones para cada modelo evaluado en la VC interna\n",
    "        #El que tenga mejor media será el usado para entrenar con el conjunto completo del fold externo y evaluar en su validación externa\n",
    "        inner_models = []\n",
    "        # Inicializar estructura para acumular scores\n",
    "        for config in model_configs:\n",
    "            inner_models.append({\n",
    "                \"name\": config[\"name\"],\n",
    "                \"model\": config[\"model\"],\n",
    "                \"params\": config[\"params\"],\n",
    "                \"acc\": [],\n",
    "                \"prec\": [],\n",
    "                \"rec\": [],\n",
    "                \"f1\": []\n",
    "            })\n",
    "        # Elegimos 4 pedazos para train, 2 para val\n",
    "        outer_train_idx = [(outer_iter + i) % 6 for i in range(4)]\n",
    "        outer_valid_idx = [(outer_iter + 4 + i) % 6 for i in range(2)]\n",
    "        \n",
    "        train_outer = pd.concat([outer_splits[i] for i in outer_train_idx])\n",
    "        valid_outer = pd.concat([outer_splits[i] for i in outer_valid_idx])\n",
    "        \n",
    "        # Dividir train_outer en 4 pedazos para inner CV\n",
    "        train_shuffled = train_outer.sample(frac=1, random_state=outer_iter).reset_index(drop=True)\n",
    "        indices_inner = np.array_split(train_shuffled.index, 4)\n",
    "        inner_splits = [train_shuffled.loc[idx] for idx in indices_inner]\n",
    "\n",
    "        \n",
    "        # Inner CV\n",
    "        for inner_iter in range(4):\n",
    "            inner_valid_pre = inner_splits[inner_iter]\n",
    "            inner_train_pre = pd.concat([s for j, s in enumerate(inner_splits) if j != inner_iter])\n",
    "            \n",
    "            # Preprocesar\n",
    "            df_train, df_val = preprocesado(inner_train_pre, inner_valid_pre, target)\n",
    "            #Se separan características y etiquetas para entrenamiento y validación\n",
    "            X_train = df_train.drop(columns=[target])\n",
    "            y_train = df_train[target]\n",
    "            X_val = df_val.drop(columns=[target])\n",
    "            y_val = df_val[target]\n",
    "\n",
    "\n",
    "            # Bucle que recorre los modelos con sus configuraciones guardadas en model_configs\n",
    "            for i, config in enumerate(model_configs):\n",
    "                # Inicializar el modelo con los parámetros de la configuración\n",
    "                modelo = config[\"model\"](**config[\"params\"])\n",
    "                \n",
    "                # Entrenar el modelo y evaluar en validación interna\n",
    "                modelo = entrenar_o_cargar(modelo, X_train, y_train, f\"results/modelo_{config['name']}_outer{outer_iter}_inner{inner_iter}_params{tuple(config['params'].values())}.pkl\")\n",
    "                preds = modelo.predict(X_val)\n",
    "                acc = accuracy_score(y_val, preds)\n",
    "                prec = precision_score(y_val, preds)\n",
    "                rec = recall_score(y_val, preds)\n",
    "                f1 = f1_score(y_val, preds)\n",
    "                \n",
    "                # Guardar las puntuaciones obtenidas para esta configuración\n",
    "                inner_models[i][\"acc\"].append(acc)\n",
    "                inner_models[i][\"prec\"].append(prec)\n",
    "                inner_models[i][\"rec\"].append(rec)\n",
    "                inner_models[i][\"f1\"].append(f1)\n",
    "\n",
    "        for model in inner_models:\n",
    "            model[\"mean_acc\"] = np.mean(model[\"acc\"])\n",
    "            model[\"mean_prec\"] = np.mean(model[\"prec\"])\n",
    "            model[\"mean_rec\"] = np.mean(model[\"rec\"])\n",
    "            model[\"mean_f1\"] = np.mean(model[\"f1\"])\n",
    "\n",
    "        inner_models_sorted = sorted(inner_models, key=lambda x: x[\"name\"])\n",
    "\n",
    "        best_models_inner = []\n",
    "\n",
    "        for name, group in groupby(inner_models_sorted, key=lambda x: x[\"name\"]):\n",
    "            group_list = list(group)\n",
    "            best_model = max(group_list, key=lambda x: x[\"mean_f1\"])\n",
    "            best_models_inner.append(best_model)\n",
    "\n",
    "        # Preprocesar outer validation usando train outer\n",
    "        train_outer_proc, valid_outer_proc = preprocesado(train_outer, valid_outer, target)\n",
    "        # Entrenar el mejor modelo en todo el outer train y evaluar en outer valid\n",
    "        X_train_outer = train_outer_proc.drop(columns=[target])\n",
    "        y_train_outer = train_outer_proc[target]\n",
    "        X_valid_outer = valid_outer_proc.drop(columns=[target])\n",
    "        y_valid_outer = valid_outer_proc[target]\n",
    "\n",
    "        # Entrenar los mejores modelos con los datos del outer train\n",
    "        for best_model in best_models_inner:\n",
    "            print(f\"Outer Iteration {outer_iter}, Evaluating best model from inner CV: {best_model['name']} with params {best_model['params']}\")\n",
    "            modelo_outer = best_model[\"model\"](**best_model[\"params\"])\n",
    "            modelo_outer = entrenar_o_cargar(modelo_outer, X_train_outer, y_train_outer, f\"results/modelo_{best_model['name']}_outer{outer_iter}_params{tuple(best_model['params'].values())}.pkl\")\n",
    "            preds = modelo_outer.predict(X_valid_outer)\n",
    "            acc = accuracy_score(y_valid_outer, preds)\n",
    "            prec = precision_score(y_valid_outer, preds)\n",
    "            rec = recall_score(y_valid_outer, preds)\n",
    "            f1 = f1_score(y_valid_outer, preds)\n",
    "            results.append({\n",
    "                \"outer_iter\": outer_iter,\n",
    "                \"name\": best_model[\"name\"],\n",
    "                \"model\": f\"modelo_{best_model['name']}_outer{outer_iter}_params{tuple(best_model['params'].values())}.pkl\",\n",
    "                \"acc\": acc,\n",
    "                \"prec\": prec,\n",
    "                \"rec\": rec,\n",
    "                \"f1\": f1\n",
    "            })\n",
    "    best_overall_models = []\n",
    "    # Escoger las mejores configuraciones de cada modelo según la métrica F1 en la validación externa\n",
    "    for name, group in groupby(sorted(results, key=lambda x: x[\"name\"]), key=lambda x: x[\"name\"]):\n",
    "        group_list = list(group)\n",
    "        best_model = max(group_list, key=lambda x: x[\"f1\"])\n",
    "        best_overall_models.append(best_model)\n",
    "    return best_overall_models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "e32b050a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outer Iteration 0, Evaluating best model from inner CV: PerceptronMulticapa with params {'hidden_layer_sizes': (50, 50), 'learning_rate_init': 0.001, 'max_iter': 1500, 'random_state': 0, 'solver': 'lbfgs'}\n",
      "Outer Iteration 0, Evaluating best model from inner CV: RegrsionLoxistica with params {'l1_ratio': 1, 'penalty': 'elasticnet', 'C': 10.0, 'solver': 'saga', 'random_state': 0}\n",
      "Outer Iteration 0, Evaluating best model from inner CV: decision_tree with params {'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 5}\n",
      "Outer Iteration 0, Evaluating best model from inner CV: knn with params {'n_neighbors': 3}\n",
      "Outer Iteration 1, Evaluating best model from inner CV: PerceptronMulticapa with params {'hidden_layer_sizes': (50, 50), 'learning_rate_init': 0.001, 'max_iter': 1500, 'random_state': 0, 'solver': 'lbfgs'}\n",
      "Outer Iteration 1, Evaluating best model from inner CV: RegrsionLoxistica with params {'l1_ratio': 1, 'penalty': 'elasticnet', 'C': 10.0, 'solver': 'saga', 'random_state': 0}\n",
      "Outer Iteration 1, Evaluating best model from inner CV: decision_tree with params {'max_depth': 10, 'min_samples_split': 10, 'min_samples_leaf': 1}\n",
      "Outer Iteration 1, Evaluating best model from inner CV: knn with params {'n_neighbors': 3}\n",
      "Outer Iteration 2, Evaluating best model from inner CV: PerceptronMulticapa with params {'hidden_layer_sizes': (100,), 'learning_rate_init': 0.001, 'max_iter': 1500, 'random_state': 0, 'solver': 'lbfgs'}\n",
      "Outer Iteration 2, Evaluating best model from inner CV: RegrsionLoxistica with params {'l1_ratio': 0, 'penalty': 'elasticnet', 'C': 10.0, 'solver': 'saga', 'random_state': 0}\n",
      "Outer Iteration 2, Evaluating best model from inner CV: decision_tree with params {'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 1}\n",
      "Outer Iteration 2, Evaluating best model from inner CV: knn with params {'n_neighbors': 5}\n"
     ]
    }
   ],
   "source": [
    "mejores_modelos = nested_cv(pd.read_csv(\"data/train.csv\"), target='calidad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "29cfb3be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'outer_iter': 1,\n",
       "  'name': 'PerceptronMulticapa',\n",
       "  'model': \"modelo_PerceptronMulticapa_outer1_params((50, 50), 0.001, 1500, 0, 'lbfgs').pkl\",\n",
       "  'acc': 0.7773311897106109,\n",
       "  'prec': 0.5230263157894737,\n",
       "  'rec': 0.5463917525773195,\n",
       "  'f1': 0.534453781512605},\n",
       " {'outer_iter': 2,\n",
       "  'name': 'RegrsionLoxistica',\n",
       "  'model': \"modelo_RegrsionLoxistica_outer2_params(0, 'elasticnet', 10.0, 'saga', 0).pkl\",\n",
       "  'acc': 0.7942122186495176,\n",
       "  'prec': 0.6048387096774194,\n",
       "  'rec': 0.26595744680851063,\n",
       "  'f1': 0.3694581280788177},\n",
       " {'outer_iter': 0,\n",
       "  'name': 'decision_tree',\n",
       "  'model': 'modelo_decision_tree_outer0_params(10, 2, 5).pkl',\n",
       "  'acc': 0.772508038585209,\n",
       "  'prec': 0.4772727272727273,\n",
       "  'rec': 0.46494464944649444,\n",
       "  'f1': 0.47102803738317756},\n",
       " {'outer_iter': 1,\n",
       "  'name': 'knn',\n",
       "  'model': 'modelo_knn_outer1_params(3,).pkl',\n",
       "  'acc': 0.7934083601286174,\n",
       "  'prec': 0.5739130434782609,\n",
       "  'rec': 0.4536082474226804,\n",
       "  'f1': 0.5067178502879078}]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mejores_modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "77a25348",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ejecucion_prediccion(file_train, file_test, configuracion_modelo):\n",
    "    df_train = pd.read_csv(file_train)\n",
    "    df_test = pd.read_csv(file_test)\n",
    "    \n",
    "    # preprocesado inicial\n",
    "    df_train = eliminacion_duplicados(df_train)\n",
    "    df_train = binarizar_calidad(df_train)\n",
    "    df_test = eliminacion_duplicados(df_test)\n",
    "    df_test = binarizar_calidad(df_test)\n",
    "    \n",
    "    # Preprocesar\n",
    "    df_train_proc, df_test_proc = preprocesado(df_train, df_test, target='calidad')\n",
    "    # Separar características y etiquetas\n",
    "    X_train = df_train_proc.drop(columns=['calidad'])\n",
    "    y_train = df_train_proc['calidad']\n",
    "    X_test = df_test_proc.drop(columns=['calidad'])\n",
    "    y_test = df_test_proc['calidad']\n",
    "\n",
    "    # Entrenar el modelo con la configuración dada\n",
    "    modelo = configuracion_modelo[\"model\"](**configuracion_modelo[\"params\"])\n",
    "    modelo = entrenar_o_cargar(modelo, X_train, y_train, f\"results/modelo_final_{configuracion_modelo['name']}_params{tuple(configuracion_modelo['params'].values())}.pkl\")\n",
    "\n",
    "    # Hacer predicciones\n",
    "    y_pred = modelo.predict(X_test)\n",
    "    # Calcular métricas\n",
    "    resultados = []\n",
    "    resultados.append({\"accuracy\": accuracy_score(y_test, y_pred)})\n",
    "    resultados.append({\"precision\": precision_score(y_test, y_pred)})\n",
    "    resultados.append({\"recall\": recall_score(y_test, y_pred)})\n",
    "    resultados.append({\"f1_score\": f1_score(y_test, y_pred)})\n",
    "    return resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "a957ee83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'accuracy': 0.8871918542336549},\n",
       " {'precision': 0.7704225352112676},\n",
       " {'recall': 0.6795031055900621},\n",
       " {'f1_score': 0.7221122112211221}]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ejecucion_prediccion(\"data/train.csv\", \"data/train.csv\", model_configs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba975dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
