{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5505af4",
   "metadata": {},
   "source": [
    "# PRÁCTICA 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f4a760",
   "metadata": {},
   "source": [
    "Lucía Pérez González, Manuel Ramallo Blanco, Alexandre Lorenzo Martínez"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b54141",
   "metadata": {},
   "source": [
    "## 1 - Preprocesado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbe9e71",
   "metadata": {},
   "source": [
    "### 1.1 - Eliminación de duplicados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b5f96fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abrir datasets\n",
    "import pandas as pd\n",
    "\n",
    "df_vino = pd.read_csv(\"data/train.csv\")\n",
    "\n",
    "# Eliminación de duplicados, ignorado quality\n",
    "cols = df_vino.columns.drop('quality')\n",
    "df_vino = df_vino.drop_duplicates(subset=cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa0834f",
   "metadata": {},
   "source": [
    "### 1.2 - Binarización de la calidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f3f78ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clasificar_vino(valor): \n",
    "    if valor < 7: \n",
    "        return 0\n",
    "    elif valor >= 7: \n",
    "        return 1 \n",
    "df_vino['calidad'] = df_vino['quality'].apply(clasificar_vino) \n",
    "df_vino = df_vino.drop(columns=['quality'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219ae80f",
   "metadata": {},
   "source": [
    "### 1.3 - Gestión de valores atípicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35d8d492",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deteccion de valores atipicos\n",
    "\n",
    "def detectar_atipicos(df_train, df_val, cols_diana):\n",
    "    for col in cols_diana:\n",
    "\n",
    "        # Calculamos IQR y límites\n",
    "        Q1 = df_train[col].quantile(0.25)\n",
    "        Q3 = df_train[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower = Q1 - 6 * IQR\n",
    "        upper = Q3 + 6 * IQR\n",
    "        \n",
    "        # Declaramos condición de atípico y registramos sus posiciones\n",
    "        cond_iqr = (df_train[col] < lower) | (df_train[col] > upper)    #OLLO! Nico recomendara ver os casos 1 a 1 \n",
    "        cond_neg = df_train[col] < 0\n",
    "        cond_atipico = cond_iqr | cond_neg\n",
    "\n",
    "        cond_iqr_val = (df_val[col] < lower) | (df_val[col] > upper)    #OLLO! Nico recomendara ver os casos 1 a 1 \n",
    "        cond_neg_val = df_val[col] < 0\n",
    "        cond_atipico_val = cond_iqr_val | cond_neg_val\n",
    "\n",
    "        # Incrementamos el contador de atípicos por fila en ambos conjuntos\n",
    "        df_train.loc[cond_atipico, col] = pd.NA\n",
    "        df_val.loc[cond_atipico_val, col] = pd.NA\n",
    "\n",
    "\n",
    "    return df_train, df_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c7c55f",
   "metadata": {},
   "source": [
    "#### 1.3.11 - Tratamiento de datos atípicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02d19cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eliminar_o_imputacion(df_train, df_val, cols_diana, max_atipicos=3, umbral_col=20,  target='calidad'):\n",
    "    # Eliminar filas con 4 o más valores atípicos\n",
    "    # Contamos el número de valores atípicos por fila (NA) y filtramos \n",
    "    df_train = df_train[df_train.isnull().sum(axis=1) < max_atipicos]\n",
    "\n",
    "    # Analisis de valores faltantes por columna\n",
    "    for col in cols_diana:\n",
    "        # Contamos el numero de valores faltantes (NA) en la columna(\n",
    "        num_faltantes = df_train[col].isna().sum()\n",
    "        if num_faltantes/len(df_train) > umbral_col:\n",
    "            df_train = df_train.drop(columns=[col])\n",
    "            df_val = df_val.drop(columns=[col])\n",
    "\n",
    "    # Imputacion de valores: mediana para cada nulo de cada columna\n",
    "    for col in cols_diana:\n",
    "        med = df_train[col].median()\n",
    "        df_train.loc[:, col] = df_train[col].fillna(med)\n",
    "        df_val.loc[:, col] = df_val[col].fillna(med)\n",
    "\n",
    "    return df_train, df_val\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d17c98",
   "metadata": {},
   "source": [
    "#### Normalización y selección de características"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be145b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalización de los datos\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def estandarizar_train_test(train_df, test_df, target):\n",
    "    columnas = train_df.drop(columns=[target]).select_dtypes(include=\"number\").columns\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "\n",
    "    train_df[columnas] = scaler.fit_transform(train_df[columnas])\n",
    "    test_df[columnas] = scaler.transform(test_df[columnas])\n",
    "\n",
    "    return train_df, test_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd31d392",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selección de características con SelectKBest\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "\n",
    "def seleccion_caracteristicas(df, k, target='calidad'):\n",
    "    X = df.drop(columns=[target])\n",
    "    y = df[target]\n",
    "\n",
    "    selector = SelectKBest(score_func=f_regression, k=k)\n",
    "    selector.fit(X, y)\n",
    "\n",
    "    selected_features = X.columns[selector.get_support()]\n",
    "\n",
    "    df_fs = df[selected_features.tolist() + [target]]\n",
    "    return df_fs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5187dd",
   "metadata": {},
   "source": [
    "#### Función que empaqueta el preprocesado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f4a658d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocesado(train_df_pre, valid_df_pre, target='calidad'):\n",
    "    df_train = train_df_pre.copy()\n",
    "    df_val = valid_df_pre.copy()\n",
    "    cols_diana = [c for c in train_df_pre.columns if c not in [target]]\n",
    "    \n",
    "    # Detección de atípicos en train y valid\n",
    "    df_train, df_val = detectar_atipicos(df_train, df_val, cols_diana)\n",
    "\n",
    "    # Tratamiento de atípicos: eliminación o imputación\n",
    "    df_train, df_val = eliminar_o_imputacion(df_train, df_val, cols_diana)\n",
    "\n",
    "    # Normalización de los datos\n",
    "    df_train, df_val = estandarizar_train_test(df_train, df_val, target)\n",
    "\n",
    "    # Selección de características con SelectKBest\n",
    "    df_train = seleccion_caracteristicas(df_train, k=10, target=target)\n",
    "    df_val = df_val[df_train.columns]\n",
    "    return df_train, df_val \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898266ff",
   "metadata": {},
   "source": [
    "## Entrenamiento del modelo de predicción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2563444f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c959d463",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "def entrenar_o_cargar(modelo, X_train, y_train, nombre_modelo_archivo):\n",
    "    #Cargar el modelo si existe\n",
    "    if os.path.exists(nombre_modelo_archivo):\n",
    "        with open(nombre_modelo_archivo, \"rb\") as f:\n",
    "            modelo = pickle.load(f)\n",
    "    #Si no existe, entrenar el modelo y guardarlo\n",
    "    else:\n",
    "        modelo.fit(X_train, y_train)\n",
    "        with open(nombre_modelo_archivo, \"wb\") as f:\n",
    "            pickle.dump(modelo, f)\n",
    "    \n",
    "    return modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ab17ac41",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "model_configs = [\n",
    "\n",
    "    # ======================\n",
    "    # kNN\n",
    "    # ======================\n",
    "    {\"name\": \"knn\", \"model\": KNeighborsClassifier, \"params\": {\"n_neighbors\": 3}},\n",
    "    {\"name\": \"knn\", \"model\": KNeighborsClassifier, \"params\": {\"n_neighbors\": 5}},\n",
    "    {\"name\": \"knn\", \"model\": KNeighborsClassifier, \"params\": {\"n_neighbors\": 7}},\n",
    "    {\"name\": \"knn\", \"model\": KNeighborsClassifier, \"params\": {\"n_neighbors\": 9}},\n",
    "\n",
    "    # ======================\n",
    "    # Árbol de Decisión\n",
    "    # ======================\n",
    "    \n",
    "    {\"name\": \"decision_tree\", \"model\": DecisionTreeClassifier,\n",
    "    \"params\": {\"max_depth\": 5, \"min_samples_split\": 2, \"min_samples_leaf\": 1}},\n",
    "\n",
    "    {\"name\": \"decision_tree\", \"model\": DecisionTreeClassifier,\n",
    "    \"params\": {\"max_depth\": 5, \"min_samples_split\": 2, \"min_samples_leaf\": 5}},\n",
    "\n",
    "    {\"name\": \"decision_tree\", \"model\": DecisionTreeClassifier,\n",
    "    \"params\": {\"max_depth\": 5, \"min_samples_split\": 10, \"min_samples_leaf\": 1}},\n",
    "\n",
    "    {\"name\": \"decision_tree\", \"model\": DecisionTreeClassifier,\n",
    "    \"params\": {\"max_depth\": 5, \"min_samples_split\": 10, \"min_samples_leaf\": 5}},\n",
    "\n",
    "    {\"name\": \"decision_tree\", \"model\": DecisionTreeClassifier,\n",
    "    \"params\": {\"max_depth\": 10, \"min_samples_split\": 2, \"min_samples_leaf\": 1}},\n",
    "\n",
    "    {\"name\": \"decision_tree\", \"model\": DecisionTreeClassifier,\n",
    "    \"params\": {\"max_depth\": 10, \"min_samples_split\": 2, \"min_samples_leaf\": 5}},\n",
    "\n",
    "    {\"name\": \"decision_tree\", \"model\": DecisionTreeClassifier,\n",
    "    \"params\": {\"max_depth\": 10, \"min_samples_split\": 10, \"min_samples_leaf\": 1}},\n",
    "\n",
    "    {\"name\": \"decision_tree\", \"model\": DecisionTreeClassifier,\n",
    "    \"params\": {\"max_depth\": 10, \"min_samples_split\": 10, \"min_samples_leaf\": 5}},\n",
    "\n",
    "    # ======================\n",
    "     # Regresión loxística\n",
    "     # ======================\n",
    "    {\"name\": \"RegrsionLoxistica\", \"model\": LogisticRegression, \"params\": {\"penalty\": \"l1\", \"C\": 0.1, \"solver\": \"liblinear\",\"random_state\": 0}},\n",
    "    {\"name\": \"RegrsionLoxistica\", \"model\": LogisticRegression, \"params\": {\"penalty\": \"l1\", \"C\": 1.0, \"solver\": \"liblinear\",\"random_state\": 0}},\n",
    "    {\"name\": \"RegrsionLoxistica\", \"model\": LogisticRegression, \"params\": {\"penalty\": \"l1\", \"C\": 10.0, \"solver\": \"liblinear\",\"random_state\": 0}},\n",
    "    {\"name\": \"RegrsionLoxistica\", \"model\": LogisticRegression, \"params\": {\"penalty\": \"l2\", \"C\": 0.1, \"solver\": \"liblinear\",\"random_state\": 0}},\n",
    "    {\"name\": \"RegrsionLoxistica\", \"model\": LogisticRegression, \"params\": {\"penalty\": \"l2\", \"C\": 1.0, \"solver\": \"liblinear\",\"random_state\": 0}},\n",
    "    {\"name\": \"RegrsionLoxistica\", \"model\": LogisticRegression, \"params\": {\"penalty\": \"l2\", \"C\": 10.0, \"solver\": \"liblinear\",\"random_state\": 0}},\n",
    "    \n",
    "    # ======================\n",
    "    # Perceptrón multicapa\n",
    "    # ======================\n",
    "    {\"name\": \"PerceptronMulticapa\", \"model\": MLPClassifier, \"params\": {\"hidden_layer_sizes\": (50,), \n",
    "                                                                       \"learning_rate_init\": 0.001, \"max_iter\": 300, \"random_state\": 0, \"solver\": \"lbfgs\"}},\n",
    "    {\"name\": \"PerceptronMulticapa\", \"model\": MLPClassifier, \"params\": {\"hidden_layer_sizes\": (50,), \n",
    "                                                                       \"learning_rate_init\": 0.01, \"max_iter\": 300, \"random_state\": 0, \"solver\": \"lbfgs\"}},\n",
    "    {\"name\": \"PerceptronMulticapa\", \"model\": MLPClassifier, \"params\": {\"hidden_layer_sizes\": (100,), \n",
    "                                                                       \"learning_rate_init\": 0.001, \"max_iter\": 300, \"random_state\": 0, \"solver\": \"lbfgs\"}},\n",
    "    {\"name\": \"PerceptronMulticapa\", \"model\": MLPClassifier, \"params\": {\"hidden_layer_sizes\": (100,), \n",
    "                                                                       \"learning_rate_init\": 0.01, \"max_iter\": 300, \"random_state\": 0, \"solver\": \"lbfgs\"}},\n",
    "    {\"name\": \"PerceptronMulticapa\", \"model\": MLPClassifier, \"params\": {\"hidden_layer_sizes\": (50, 50), \n",
    "                                                                       \"learning_rate_init\": 0.001, \"max_iter\": 300, \"random_state\": 0, \"solver\": \"lbfgs\"}},\n",
    "    {\"name\": \"PerceptronMulticapa\", \"model\": MLPClassifier, \"params\": {\"hidden_layer_sizes\": (50, 50), \n",
    "                                                                       \"learning_rate_init\": 0.01, \"max_iter\": 300, \"random_state\": 0, \"solver\": \"lbfgs\"}}\n",
    "]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2fead309",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo de entrenamiento\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def nested_cv(df, target='calidad'):\n",
    "    \"\"\"\n",
    "    Validación cruzada anidada:\n",
    "    - exterior: 3 iteraciones (6 pedazos, 4 train, 2 val)\n",
    "    - interior: 4 pedazos (3 train, 1 val)\n",
    "    \n",
    "    Devuelve: diccionario con información de folds\n",
    "    \"\"\"\n",
    "    \n",
    "    # Mezclar el dataset para aleatoriedad\n",
    "    df_shuffled = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    n = len(df_shuffled)\n",
    "    \n",
    "    # Dividir en 6 pedazos iguales (outer)\n",
    "    outer_splits = [ df_shuffled.iloc[i::6] for i in range(6) ]\n",
    "    \n",
    "    results = []\n",
    "\n",
    "    # Outer CV\n",
    "    for outer_iter in range(3):\n",
    "        #Se construye el diccionario para cada iteración externa -> se almacena un array de puntuaciones para cada modelo evaluado en la VC interna\n",
    "        #El que tenga mejor media será el usado para entrenar con el conjunto completo del fold externo y evaluar en su validación externa\n",
    "        inner_models = []\n",
    "        # Inicializar estructura para acumular scores\n",
    "        for config in model_configs:\n",
    "            inner_models.append({\n",
    "                \"name\": config[\"name\"],\n",
    "                \"model\": config[\"model\"],\n",
    "                \"params\": config[\"params\"],\n",
    "                \"acc\": [],\n",
    "                \"prec\": [],\n",
    "                \"rec\": [],\n",
    "                \"f1\": []\n",
    "            })\n",
    "        # Elegimos 4 pedazos para train, 2 para val\n",
    "        outer_train_idx = [(outer_iter + i) % 6 for i in range(4)]\n",
    "        outer_valid_idx = [(outer_iter + 4 + i) % 6 for i in range(2)]\n",
    "        \n",
    "        train_outer = pd.concat([outer_splits[i] for i in outer_train_idx])\n",
    "        valid_outer = pd.concat([outer_splits[i] for i in outer_valid_idx])\n",
    "        \n",
    "        # Dividir train_outer en 4 pedazos para inner CV\n",
    "        train_shuffled = train_outer.sample(frac=1, random_state=outer_iter).reset_index(drop=True)\n",
    "        indices_inner = np.array_split(train_shuffled.index, 4)\n",
    "        inner_splits = [train_shuffled.loc[idx] for idx in indices_inner]\n",
    "\n",
    "        \n",
    "        # Inner CV\n",
    "        for inner_iter in range(4):\n",
    "            inner_valid_pre = inner_splits[inner_iter]\n",
    "            inner_train_pre = pd.concat([s for j, s in enumerate(inner_splits) if j != inner_iter])\n",
    "            \n",
    "            # Preprocesar\n",
    "            df_train, df_val = preprocesado(inner_train_pre, inner_valid_pre, target)\n",
    "            #Se separan características y etiquetas para entrenamiento y validación\n",
    "            X_train = df_train.drop(columns=[target])\n",
    "            y_train = df_train[target]\n",
    "            X_val = df_val.drop(columns=[target])\n",
    "            y_val = df_val[target]\n",
    "\n",
    "\n",
    "            # Bucle que recorre los modelos con sus configuraciones guardadas en model_configs\n",
    "            for i, config in enumerate(model_configs):\n",
    "                # Inicializar el modelo con los parámetros de la configuración\n",
    "                modelo = config[\"model\"](**config[\"params\"])\n",
    "                \n",
    "                # Entrenar el modelo y evaluar en validación interna\n",
    "                modelo = entrenar_o_cargar(modelo, X_train, y_train, f\"results/modelo_{config['name']}_outer{outer_iter}_inner{inner_iter}_params{tuple(config['params'].values())}.pkl\")\n",
    "                preds = modelo.predict(X_val)\n",
    "                acc = accuracy_score(y_val, preds)\n",
    "                prec = precision_score(y_val, preds)\n",
    "                rec = recall_score(y_val, preds)\n",
    "                f1 = f1_score(y_val, preds)\n",
    "                \n",
    "                # Guardar las puntuaciones obtenidas para esta configuración\n",
    "                inner_models[i][\"acc\"].append(acc)\n",
    "                inner_models[i][\"prec\"].append(prec)\n",
    "                inner_models[i][\"rec\"].append(rec)\n",
    "                inner_models[i][\"f1\"].append(f1)\n",
    "\n",
    "        for model in inner_models:\n",
    "            model[\"mean_acc\"] = np.mean(model[\"acc\"])\n",
    "            model[\"mean_prec\"] = np.mean(model[\"prec\"])\n",
    "            model[\"mean_rec\"] = np.mean(model[\"rec\"])\n",
    "            model[\"mean_f1\"] = np.mean(model[\"f1\"])\n",
    "        best_model = max(inner_models, key=lambda x: x[\"mean_f1\"])\n",
    "\n",
    "        # Preprocesar outer validation usando train outer\n",
    "        train_outer_proc, valid_outer_proc = preprocesado(train_outer, valid_outer, target)\n",
    "        # Entrenar el mejor modelo en todo el outer train y evaluar en outer valid\n",
    "        X_train_outer = train_outer_proc.drop(columns=[target])\n",
    "        y_train_outer = train_outer_proc[target]\n",
    "        X_valid_outer = valid_outer_proc.drop(columns=[target])\n",
    "        y_valid_outer = valid_outer_proc[target]\n",
    "\n",
    "        # Entrenar el mejor modelo con los datos del outer train\n",
    "        modelo_outer = best_model[\"model\"](**best_model[\"params\"])\n",
    "        modelo_outer = entrenar_o_cargar(modelo_outer, X_train_outer, y_train_outer, f\"results/modelo_{best_model['name']}_outer{outer_iter}_params{tuple(best_model['params'].values())}.pkl\")\n",
    "        preds = modelo_outer.predict(X_valid_outer)\n",
    "        acc = accuracy_score(y_valid_outer, preds)\n",
    "        prec = precision_score(y_valid_outer, preds)\n",
    "        rec = recall_score(y_valid_outer, preds)\n",
    "        f1 = f1_score(y_valid_outer, preds)\n",
    "        results.append({\n",
    "            \"outer_iter\": outer_iter,\n",
    "            \"best_model\": best_model[\"name\"]+str(best_model[\"params\"]),\n",
    "            \"acc\": acc,\n",
    "            \"prec\": prec,\n",
    "            \"rec\": rec,\n",
    "            \"f1\": f1\n",
    "        })\n",
    "    best_overall_model = max(results, key=lambda x: x[\"f1\"])\n",
    "    return best_overall_model, results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e32b050a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mejor_resultado,resultados = nested_cv(df_vino, target='calidad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "29cfb3be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'outer_iter': 1,\n",
       " 'best_model': \"PerceptronMulticapa{'hidden_layer_sizes': (50, 50), 'learning_rate_init': 0.001, 'max_iter': 300, 'random_state': 0, 'solver': 'lbfgs'}\",\n",
       " 'acc': 0.7773311897106109,\n",
       " 'prec': 0.5230263157894737,\n",
       " 'rec': 0.5463917525773195,\n",
       " 'f1': 0.534453781512605}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mejor_resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a25348",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
